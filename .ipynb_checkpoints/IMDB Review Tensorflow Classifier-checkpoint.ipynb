{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4e48dd",
   "metadata": {},
   "source": [
    "<img src=\"images/imdb_logo.png\" width=\"200\" height=\"200\" />\n",
    "\n",
    "# IMDB Review Tensorflow Classifier\n",
    "\n",
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification.It has a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4c019",
   "metadata": {},
   "source": [
    "Import all the libraries I would need. Keras will provide the IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b15baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8dbf2",
   "metadata": {},
   "source": [
    "The argument **num_words=10000** means you’ll only keep the top 10 000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size.\n",
    "As stated above the dataset should contain 25k reviews of movies which are the rows of the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93081d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is  (25000,)\n",
      "train_labels shape is  (25000,)\n",
      "test_data shape is  (25000,)\n",
      "test_labels shape is  (25000,)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "print(\"train_data shape is \", train_data.shape)\n",
    "print(\"train_labels shape is \", train_labels.shape)\n",
    "print(\"test_data shape is \", train_data.shape)\n",
    "print(\"test_labels shape is \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b612b3",
   "metadata": {},
   "source": [
    "Each row contains indexes of the word position in the dictionary and the label is either 1 or 0 to state whether the review was positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bbd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 22, 2013, 19, 6, 6704, 324, 7, 6, 2040, 860, 313, 601, 19, 2, 5370, 15, 4474, 23, 4, 499, 7, 6, 392, 2190, 5, 35, 23, 268, 2, 739, 15, 4535, 2, 2, 1898, 2, 6, 185, 860, 2, 3632, 773, 2, 2, 2, 2371, 56, 4, 2, 2524, 8, 4, 313, 1004, 9313, 2, 2, 2, 19, 937, 29, 9, 260, 35, 1586, 496, 41, 658, 2, 2, 2, 17, 2, 2, 145, 37, 571, 8, 30, 2, 1750, 2, 5167, 2957, 344, 8, 169, 27, 322, 5, 1475, 260, 55, 4691, 4249, 19, 257, 85, 27, 9658, 2730, 4, 2, 3632, 4653, 1496, 199, 2, 5, 2, 159, 8000, 1720, 120, 6, 1117, 303, 5, 2901, 2, 2498, 2060, 2957, 11, 1898, 23, 6, 780, 3182, 19, 27, 322, 3021, 2787, 742, 5, 68, 185, 577, 4232, 4232, 2, 68, 491, 464, 2365, 3378, 7517, 2, 37, 495, 18, 4, 298, 2, 1525, 98, 46, 34, 1556, 98, 6, 273, 8, 789, 25, 92, 359, 72, 8, 25, 121, 29, 2, 560, 45, 170, 38, 706, 88, 45, 2368, 8, 63, 199, 2901, 5, 3021, 462, 125, 17, 36, 540, 92, 264, 11, 2745, 33, 222, 18, 4, 58, 112, 15, 9, 220, 1241, 4, 22, 271, 83, 1591, 5440, 2690, 471, 23, 5, 125, 34, 533, 3021, 47, 35, 7971, 547, 44, 4, 273, 5, 6, 719, 2, 4932, 1469, 2, 408, 98, 35, 6559, 1710, 18, 98, 8, 563, 159, 45, 99, 522, 63, 36, 2754, 7, 265, 4, 4123, 7, 2, 2, 5, 2, 71, 4162, 18, 5796, 8, 2416, 746, 4, 3404, 7, 4, 313, 88, 7, 6, 2, 4701, 2, 2, 3176, 276, 725, 98, 21, 50, 203, 30, 6, 96, 36, 70, 989, 4, 3176, 472, 18, 4, 2, 223, 12, 100, 4616, 2322, 98, 68, 1349, 577, 5, 869, 60, 68, 55, 456, 10, 10, 526, 34, 1842, 9405, 13, 197, 14, 16, 6, 184, 856, 22, 608, 8, 106, 280, 48, 874, 188, 164, 128, 8, 81, 21, 103, 6, 251, 42, 107, 490, 242, 28, 340, 1551, 12, 164, 3775, 11, 4, 1757, 17, 112, 572, 78, 21, 23, 4, 85, 508, 225, 164, 572, 52, 44, 4, 22, 345, 4, 229, 34, 670, 2, 39, 4, 667, 34, 592, 2, 9, 6, 117, 23, 4, 753, 499, 5, 3481, 34, 4, 1396, 6, 176, 7, 8046, 7787, 23, 593, 469, 4, 22, 21, 600, 7, 12, 9, 55, 221, 42, 1127, 5, 4, 1035, 105, 5, 458, 152, 339, 183, 50, 26, 378, 7, 710, 844, 40, 4, 1463, 3161, 2, 15, 353, 8, 79, 4232, 5, 41, 9773, 2, 2, 2, 5, 50, 9, 6, 136, 121, 4, 2, 4932, 2, 4, 313, 5, 4, 2745, 26, 2, 1005, 2103, 8, 79, 145, 11, 190, 15, 9, 366, 2901, 331, 2013, 4, 1312, 5, 36, 43, 1135, 208, 145, 11, 49, 2, 31, 53, 155, 13, 104, 12, 16, 6, 78, 326, 8, 28, 7517, 2, 37, 16, 2, 54, 29, 93, 14, 353, 27, 508, 33, 2135, 1879, 5, 9618, 2554, 997, 742, 214, 41, 7411, 3507, 46, 6, 378, 7, 211, 587, 6, 55, 2, 383, 136, 19, 2, 261, 87, 7154, 5, 6, 378, 7, 1445, 7867, 15, 1231, 8, 98, 40, 1165, 2, 71, 623, 8, 6584, 57, 1908, 4, 2, 1006, 16, 984, 972, 39, 6, 378, 7, 669, 125, 268, 2, 225, 57, 541, 599, 42, 567, 8, 1128, 44, 4, 2, 1213, 302, 26, 608, 21, 36, 2808, 170, 8, 4171, 111, 84, 134, 504, 45, 2, 195, 93, 5, 272, 179, 327, 21, 4, 986, 11, 4, 860, 956, 5, 2, 9, 2, 17, 14, 22, 100, 28, 77, 270, 11, 938, 1831, 42, 101, 1010, 704, 209, 260, 8, 653, 6, 155, 35, 608, 58, 2]\n",
      "negative review\n"
     ]
    }
   ],
   "source": [
    "random_index = random.randrange(0, train_data.shape[0])\n",
    "random_review = train_data[random_index]\n",
    "random_review_setiment = train_labels[random_index]\n",
    "\n",
    "print (random_review)\n",
    "if(random_review_setiment == 0 ):\n",
    "    print(\"negative review\")\n",
    "else:\n",
    "    print(\"positive review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f4464",
   "metadata": {},
   "source": [
    "The words are indexed based on their occurence in an ascending order, that means they would be no word that has an index greater than 9999 and that our dictionary is limited to 9997 words.\n",
    "\n",
    "Lets create a function that will decode the review from numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a4ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(review):\n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in review])\n",
    "    return decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9e895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: ? the film opens with a peaceful shot of a traditional japanese house complete with ? roof that sits on the side of a small hill and an on screen ? appears that reads ? ? japan ? a young japanese ? samurai named ? ? ? walks up the ? path to the house inside waits ? ? ? with whom he is having an affair behind her husband ? ? ? as ? ? back who happens to be ? teacher ? unexpectedly arrives home to find his wife and student having very intimate relations with each other his honour destroyed the ? samurai brutally murders both ? and ? before committing suicide over a century later and ted ? edward albert arrives in japan on a working holiday with his wife laura susan george and their young daughter amy amy ? their close friend alex curtis doug ? who works for the american ? helps them out by finding them a place to stay you don't need me to you where he ? says it's going so cheap because it's haunted to which both ted and laura laugh off as they obviously don't believe in ghosts at least for the time being that is almost immediately the film goes into cliché mode lights turn on and off by themselves laura has an uneasy feeling about the place and a local ? monk henry ? gives them an ominous warning for them to leave before it's too late which they ignore of course the spirits of ? ? and ? were doomed for eternity to remain within the walls of the house because of a ? witches ? ? curse put upon them but there may be a way they can break the curse unfortunately for the ? family it could potentially cost them their marriage daughter and possibly even their very lives br br directed by kevin connor i thought this was a pretty average film ok to watch once if you've got nothing better to do but after a day or two you'll probably have completely forgotten it nothing sticks in the memory as being particularly bad but on the other hand there's nothing particularly good about the film either the script by robert ? from the novel by james ? is a little on the dull side and strictly by the numbers a lot of ghostly goings on happen throughout the film but none of it is very interesting or exciting and the flat characters and direction doesn't help things there are couple of silly sequences like the giant plastic ? that try to get amy and her babysitter ? ? ? and there is a scene where the ? monk ? the house and the ghosts are ? outside unable to get back in however that is until ted simply opens the door and they just walk right back in some ? one more thing i think it was a bad idea to have doug ? who was ? when he made this try his hand at kung fu and oriental sword fighting george gets her ample breasts out a couple of times including a very ? sex scene with ? although great pains and a couple of bed sheets that stick to them like super ? were taken to ensure no below the ? nudity was present apart from a couple of mostly off screen ? there's no blood gore or violence to speak about the ? ghost effects are ok but they ain't going to impress many people these days it's ? enough made and looks quite nice but the potential in the japanese setting and ? is ? as this film could have been set in america england or any western country without having to change a thing an ok time ?\n",
      "negative review\n"
     ]
    }
   ],
   "source": [
    "decoded_review = decode_review(random_review)\n",
    "print(\"review:\", decoded_review)\n",
    "if(random_review_setiment == 0 ):\n",
    "    print(\"negative review\")\n",
    "else:\n",
    "    print(\"positive review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7a015",
   "metadata": {},
   "source": [
    "We need to standardize the reviews because they each contain a different number of words, we should encode them in such a way that they all have the same length while preserving the integrity of the data.\n",
    "\n",
    "Since our diction has 10 000 words we can encode each review to a 10 000 length array where the index of each word is marked as a 1 and the rest of the array is zeros.\n",
    "\n",
    "For example, review [2, 4, 5] would be [0, 0, 1, 0, 1, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94bc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2c4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_review = vectorize_sequences(train_data[1:2])\n",
    "print(encoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b454d6e",
   "metadata": {},
   "source": [
    "## Neural Network Design\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdd41b",
   "metadata": {},
   "source": [
    "## Design Validation\n",
    "Using a subset of the data to check if our dsesign will converge. Will use 10 000 out of 25 000 datasets to verify my models works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f473411",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58edec",
   "metadata": {},
   "source": [
    "Now to implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f0a6e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10000\u001b[39m,)))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bd456",
   "metadata": {},
   "source": [
    "Lets visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6654a5a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_dict \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n\u001b[0;32m      2\u001b[0m loss_values \u001b[38;5;241m=\u001b[39m history_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m val_loss_values \u001b[38;5;241m=\u001b[39m history_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(val_loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d1c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
