{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4e48dd",
   "metadata": {},
   "source": [
    "<img src=\"images/imdb_logo.png\" width=\"200\" height=\"200\" />\n",
    "\n",
    "# IMDB Review Tensorflow Classifier\n",
    "\n",
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification.It has a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1a7b9",
   "metadata": {},
   "source": [
    "Import all the libraries I would need. Keras will provide the IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b15baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab27b05",
   "metadata": {},
   "source": [
    "The argument **num_words=10000** means youâ€™ll only keep the top 10 000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size.\n",
    "As stated above the dataset should contain 25k reviews of movies which are the rows of the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93081d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is  (25000,)\n",
      "train_labels shape is  (25000,)\n",
      "test_data shape is  (25000,)\n",
      "test_labels shape is  (25000,)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "print(\"train_data shape is \", train_data.shape)\n",
    "print(\"train_labels shape is \", train_labels.shape)\n",
    "print(\"test_data shape is \", train_data.shape)\n",
    "print(\"test_labels shape is \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558bfb37",
   "metadata": {},
   "source": [
    "Each row contains indexes of the word position in the dictionary and the label is either 1 or 0 to state whether the review was positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b97fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 260, 332, 4, 85, 795, 23, 14, 22, 13, 62, 40, 8, 1497, 61, 205, 650, 15, 14, 9, 31, 1211, 20, 8, 67, 894, 25, 26, 6, 964, 2, 13, 244, 24, 10, 10, 54, 610, 33, 34, 6, 2855, 5210, 2, 4, 22, 9, 35, 2, 1321, 15, 2423, 178, 19, 53, 2, 2, 74, 26, 2250, 7, 112, 5742, 11, 31, 1266, 12, 9, 179, 878, 8, 106, 4, 3228, 109, 670, 3636, 9, 38, 6938, 5, 4009, 15, 12, 9, 254, 8, 2198, 8, 90, 387, 584, 3663, 18, 90, 17, 29, 5905, 39, 31, 5216, 532, 5742, 904, 8, 4, 375, 5, 29, 144, 115, 81, 6, 4245, 136, 5, 535, 8, 30, 623, 615, 11, 6, 731, 2008, 57, 132, 100, 28, 15, 76, 3773, 2, 5, 30, 424, 8, 471, 23, 6, 4545, 40, 5839, 2, 894, 7, 265, 29, 9, 2060, 3228, 11, 35, 2060, 3228, 5407, 365, 10, 10, 682, 883, 47, 94, 1139, 388, 21, 36, 26, 2, 5, 2, 53, 400, 74, 24, 13, 421, 17, 48, 13, 71, 23, 6, 7616, 1311, 19, 6, 1562, 2, 415, 5, 4864, 15, 12, 62, 130, 460, 12, 2, 4, 712, 15, 70, 2061, 54, 99, 76, 1140, 7, 6, 22, 9, 2611, 11, 64, 31, 415, 294, 37, 1503, 4, 532, 7696, 8, 30, 502, 8, 1491, 145, 39, 12, 5, 67, 51, 9, 695, 1448, 10, 10, 17, 91, 84, 242, 124, 592, 2014, 3228, 37, 256, 4, 167, 11, 14, 22, 9, 11, 192, 51, 29, 1000, 1446, 153, 303, 29, 177, 2060, 3228, 11, 4, 55, 1112, 3851, 1636, 592, 1177, 178, 89, 2060, 70, 2995, 6, 3084, 5, 441, 700, 239, 2060, 1668, 178, 8, 67, 592, 1228, 24, 177, 17, 35, 284, 81, 6, 1281, 700, 471, 11, 6, 1238, 1134, 1377, 10, 10, 451, 7, 682, 883, 80, 34, 150, 28, 1681, 23, 8, 4, 375, 2, 931, 44, 12, 8, 25, 13, 135, 50, 9, 195, 1460, 11, 4, 182, 209, 260, 8, 169, 12, 11, 6, 22, 1436, 17, 35, 722]\n",
      "negative review\n"
     ]
    }
   ],
   "source": [
    "random_index = random.randrange(0, train_data.shape[0])\n",
    "random_review = train_data[random_index]\n",
    "random_review_setiment = train_labels[random_index]\n",
    "\n",
    "print (random_review)\n",
    "if(random_review_setiment == 0 ):\n",
    "    print(\"negative review\")\n",
    "else:\n",
    "    print(\"positive review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1373a7",
   "metadata": {},
   "source": [
    "The words are indexed based on their occurence in an ascending order, that means they would be no word that has an index greater than 9999 and that our dictionary is limited to 9997 words.\n",
    "\n",
    "Lets create a function that will decode the review from numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8a545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(review):\n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in review])\n",
    "    return decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82589835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: ? having read the other comments on this film i would like to share my own view that this is one tough movie to see unless you are a total ? i am not br br when looked at by a purely objective ? the film is an ? narrative that presents us with more ? ? than are capable of being absorbed in one sitting it is quite difficult to watch the brooks character robert cole is so unsympathetic and unpleasant that it is hard to relate to him let alone root for him as he stumbles from one dysfunctional self absorbed situation to the next and he should never do a topless scene and expect to be taken seriously in a romantic context no man could have that much exposed ? and be supposed to turn on a babe like kathryn ? unless of course he is albert brooks in an albert brooks controlled production br br modern romance has its amusing moments but they are ? and ? more often than not i felt as if i were on a confined journey with a thoroughly ? person and wishing that it would end already it ? the problems that can develop when too much control of a film is placed in only one person someone who lacks the self discipline to be able to step back from it and see what is clearly happening br br as most people probably know james l brooks who played the director in this film is in fact what he portrayed six years later he cast albert brooks in the very successful broadcast news james showed us how albert can shape a credible and entertaining comic performance albert allowed us to see james generally not cast as an actor do a rare comic turn in a surprisingly effective manner br br fans of modern romance will by now have moved on to the next ? comment about it to you i say there is enough pain in the world without having to find it in a film intended as an entertainment\n",
      "negative review\n"
     ]
    }
   ],
   "source": [
    "decoded_review = decode_review(random_review)\n",
    "print(\"review:\", decoded_review)\n",
    "if(random_review_setiment == 0 ):\n",
    "    print(\"negative review\")\n",
    "else:\n",
    "    print(\"positive review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc5ad3",
   "metadata": {},
   "source": [
    "We need to standardize the reviews because they each contain a different number of words, we should encode them in such a way that they all have the same length while preserving the integrity of the data.\n",
    "\n",
    "Since our diction has 10 000 words we can encode each review to a 10 000 length array where the index of each word is marked as a 1 and the rest of the array is zeros.\n",
    "\n",
    "For example, review [2, 4, 5] would be [0, 0, 1, 0, 1, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218832da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae70006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_review = vectorize_sequences(train_data[1:2])\n",
    "print(encoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d301b4",
   "metadata": {},
   "source": [
    "## Neural Network Design\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383609c",
   "metadata": {},
   "source": [
    "## Design Validation\n",
    "Using a subset of the data to check if our dsesign will converge. Will use 10 000 out of 25 000 datasets to verify my models works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f064e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f4702",
   "metadata": {},
   "source": [
    "Now to implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ad3ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.5005 - accuracy: 0.8033 - val_loss: 0.3881 - val_accuracy: 0.8592\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3029 - accuracy: 0.9031 - val_loss: 0.3046 - val_accuracy: 0.8868\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2237 - accuracy: 0.9267 - val_loss: 0.2883 - val_accuracy: 0.8856\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1803 - accuracy: 0.9436 - val_loss: 0.3031 - val_accuracy: 0.8771\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1466 - accuracy: 0.9533 - val_loss: 0.2944 - val_accuracy: 0.8831\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1184 - accuracy: 0.9629 - val_loss: 0.2936 - val_accuracy: 0.8850\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0979 - accuracy: 0.9715 - val_loss: 0.3230 - val_accuracy: 0.8772\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0835 - accuracy: 0.9755 - val_loss: 0.3490 - val_accuracy: 0.8782\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0701 - accuracy: 0.9809 - val_loss: 0.3533 - val_accuracy: 0.8806\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0580 - accuracy: 0.9850 - val_loss: 0.3832 - val_accuracy: 0.8750\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9882 - val_loss: 0.4175 - val_accuracy: 0.8743\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0371 - accuracy: 0.9921 - val_loss: 0.4536 - val_accuracy: 0.8739\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0304 - accuracy: 0.9937 - val_loss: 0.4738 - val_accuracy: 0.8740\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0269 - accuracy: 0.9946 - val_loss: 0.4961 - val_accuracy: 0.8719\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0195 - accuracy: 0.9974 - val_loss: 0.5331 - val_accuracy: 0.8689\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.5791 - val_accuracy: 0.8643\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0107 - accuracy: 0.9992 - val_loss: 0.7343 - val_accuracy: 0.8435\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0103 - accuracy: 0.9987 - val_loss: 0.6998 - val_accuracy: 0.8530\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.0084 - accuracy: 0.9994 - val_loss: 0.6770 - val_accuracy: 0.8650\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0061 - accuracy: 0.9994 - val_loss: 0.8674 - val_accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc090b",
   "metadata": {},
   "source": [
    "Lets visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f04a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(val_loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344beff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
